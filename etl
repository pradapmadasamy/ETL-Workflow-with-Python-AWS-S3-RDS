import logging
import os

# Set up logging
log_file = 'C:/Users/prada/Desktop/Newfolder/etl_pipeline.log'  
os.makedirs(os.path.dirname(log_file), exist_ok=True)  

logging.basicConfig(
    filename=log_file,
    level=logging.INFO,  # Set the logging level to INFO
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# Also log to console
console = logging.StreamHandler()
console.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console.setFormatter(formatter)
logging.getLogger('').addHandler(console)

logging.info("Logging system initialized")


import boto3
import os
import pandas as pd
import logging
import glob
from sqlalchemy import create_engine

# Create an connection with S3 client
s3_client = boto3.client(
    's3',
    aws_access_key_id='AKIAYS2NUYT4OB6V7QN2',
    aws_secret_access_key='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',
    region_name='us-east-1'
)

# Function to download files from S3
def download_files_by_extension_from_s3(bucket_name, file_types, local_dir):
    logging.info(f"Downloading files from S3 bucket: {bucket_name}")
    if not os.path.exists(local_dir):
        os.makedirs(local_dir)
        logging.info(f"Created local directory: {local_dir}")
    
    response = s3_client.list_objects_v2(Bucket=bucket_name)
    
    if 'Contents' in response:
        for item in response['Contents']:
            s3_file_key = item['Key']
            
            if s3_file_key.endswith(tuple(file_types)):
                local_file_path = os.path.join(local_dir, os.path.basename(s3_file_key))
                s3_client.download_file(bucket_name, s3_file_key, local_file_path)
                logging.info(f"Downloaded {s3_file_key} to {local_file_path}")
    else:
        logging.warning("No files found in the bucket.")

bucket_name = 'my-etl-project-bucket'
file_types = ['.csv', '.xml', '.json']
local_directory = 'C:/Users/prada/Desktop/Newfolder/raw_data'

download_files_by_extension_from_s3(bucket_name, file_types, local_directory)

# Read all raw data from the local directory and concatenate into a single dataframe
def read_and_concatenate_data(source_folder_path):
    logging.info(f"Reading data from local directory: {source_folder_path}")
    dataframes = []

    # Read CSV files
    csv_files = glob.glob(os.path.join(source_folder_path, '*.csv'))
    for file in csv_files:
        df_csv = pd.read_csv(file)
        dataframes.append(df_csv)
        logging.info(f"Read CSV file: {file}")

    # Read JSON files
    json_files = glob.glob(os.path.join(source_folder_path, '*.json'))
    for file in json_files:
        df_json = pd.read_json(file)
        dataframes.append(df_json)
        logging.info(f"Read JSON file: {file}")

    # Read XML files
    xml_files = glob.glob(os.path.join(source_folder_path, '*.xml'))
    for file in xml_files:
        df_xml = pd.read_xml(file)
        dataframes.append(df_xml)
        logging.info(f"Read XML file: {file}")

    df_con = pd.concat(dataframes, ignore_index=True)
    logging.info("Concatenated all data into a single DataFrame")
    return df_con

df = read_and_concatenate_data(local_directory)

# Transform the data
def convert_units_clean_data(df):
    logging.info("Transforming the data")
    df['height'] = df['height'] * 0.0254  # converting inches to meters
    df['weight'] = df['weight'] * 0.453592  # converting pounds to kilograms
    df = df.drop_duplicates(subset=['name', 'height', 'weight'], keep='first')  # remove duplicates
    logging.info("Transformation complete")
    return df

df = convert_units_clean_data(df)

# Save the cleaned data to a local CSV file
cleaned_data_path = 'C:/Users/prada/Desktop/Newfolder/cleaned_data.csv'
df.to_csv(cleaned_data_path, index=False)
logging.info(f"Saved cleaned data to {cleaned_data_path}")

# Upload the cleaned data to S3
def upload_to_s3(local_file_path, bucket_name, s3_file_key):
    s3_client.upload_file(local_file_path, bucket_name, s3_file_key)
    logging.info(f"Uploaded {local_file_path} to S3 bucket: {bucket_name}")

bucket_name_transformed = 'transformed--data'
upload_to_s3(cleaned_data_path, bucket_name_transformed, 'cleaned_data.csv')

# Load the cleaned data into RDS
def load_data_to_rds(df, table_name, engine):
    logging.info(f"Loading data into RDS table: {table_name}")
    df.to_sql(table_name, con=engine, if_exists='replace', index=False)
    logging.info(f"Data loaded into {table_name} successfully")

rds_host = 'etl-database.c7kmcuyw0xlz.us-east-1.rds.amazonaws.com'
rds_user = 'admin'
rds_password = 'rootroot'
rds_db = 'etl_project'
rds_port = 3306
table_name = 'ETLDATA'

engine = create_engine(f'mysql+pymysql://{rds_user}:{rds_password}@{rds_host}:{rds_port}/{rds_db}')
load_data_to_rds(df, table_name, engine)

# Fetching data from RDS
def fetch_data_from_rds(engine, query):
    logging.info("Fetching data from RDS")
    df = pd.read_sql(query, con=engine)
    logging.info("Fetched data successfully")
    return df

query = "SELECT * FROM ETLDATA"
df_rds = fetch_data_from_rds(engine, query)

logging.info("ETL pipeline completed successfully")
